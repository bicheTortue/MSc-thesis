\section{Weights generation}
\label{sec:genwei}

The choice for the weight generation was to use Keras API with the tensorflow framework. However, both tensorflow and pyTorch were tried and were giving similar results, the final choice of using tensorflow was made because it is the most popular among the research group.

\subsection{Training the weights}

Generating the weights requires to train a \ac{NN}. This is done in Keras by creating the model, in other words the architecture required to solve the problem. This model contains the description of the \ac{NN}, such as the type of \ac{RNN} and the Dense layers that would come before of after the \ac{RNN}. In Keras, there are only two of the \ac{LSTM} variants (\cref{sec:lstm}) available :

\begin{itemize}
  \item The \ac{NP} \ac{LSTM} : This is most commonly used version and by such one of the two available in Keras.
  \item The \ac{GRU} : This one is present here because it is considered to be different from an \ac{LSTM} \ac{NN} despite their similarities.
\end{itemize}

The other kinds of \ac{LSTM} are not present in the default Keras librairies.

Training can only be done outside of the circuit as this state of the project. However, in order to train the weights as closely as they would have been on a real circuit, the activation functions used for the training are the custom activation functions generated by the dedicated circuit (\cref{sec:af}). In other words, the activation functions used are the ones shown in \cref{fig:afGraph}. This allows to train the weights as they almost (considering the activation functions are recreated using 51 simulated points) as they would have been in the real circuit.

All the weight trainings are done using \ac{MSE} loss function and Adam optimizer \cite{adamOpti}.

\subsection{Weights constaint}

The weights need to be contrained during the training to make sure that the voltage of the analog circuit stays within its operating voltage range ($[0,V_{dd}]$). To avoid any unwanted behavior when the system supposedly goes out of range, the weights in the \ac{LSTM} are contrained. This constrain is not fixed and depends on the architecture it is generated with.

The worst case scenario for an \ac{LSTM} is when every value reaches the voltage threshold ($V_{threshold}$) (\cref{sec:xbarCircuit}) and every weight is maximized ($w_{max}$). When that is the case the output of the \acp{VMM} is found in \cref{eq:weiCons}.

\begin{equation}\label{eq:weiCons}
  V_{threshold}\cdot w_{max} \cdot(n_i+n_h)+w_{max}+V_{cm}= V_{dd}
\end{equation}

We can then determine the maximum acceptable value for the weights ($w_{max}$), this value has thus been determined to be the one in \cref{eq:weiConsRes}.

\begin{equation}\label{eq:weiConsRes}
  w_{max}=\frac{V_{dd}-V_{cm}}{V_{threshold}\cdot(n_i+n_h)+1}
\end{equation}

The analog system being completly centered and symetrical around $V_{cm}$ the value for the minimum weight is the opposite to the maximum one (\cref{eq:weiConsRes1})

\begin{equation}\label{eq:weiConsRes1}
  w_{min}=-w_{max}=-\frac{V_{dd}-V_{cm}}{V_{threshold}\cdot(n_i+n_h)+1}
\end{equation}

A way to remove those constraints is discussed in \cref{subsec:noCons}. Having a constraint on the weights limits the performance of the final \ac{NN}.

\subsection{Exporting the weights}

Once all the parameters (number of hidden states, number of dense layers, etc) have been set. The weights can be exported using the required weight repartition layed out in \cref{sec:netlist}. A description of the architecture is also saved along the weights. This file can now be used as the input for the netlist generator (\cref{sec:netlist}).

The code used for all the weights generations is available at \cite{lstmWei}. The code containing the finctions to save the weights to a file is available in \ac{apsec:saveWei}.
