\section{Neural Networks}\label{sec:nn}

\ac{AI} is wide topic, it refers to any computer system that can make a descision. It can take many forms but usually when we talk about \acp{AI}, we refer to the most common type, Machine Learning. Especially, a sub part of Machine Learning called Deep Learning. Deep Learning has seen a surge in research the last decade, which lead to great advancement in this field and the numerous \ac{AI} tools that popped up in the last years.
Deep Learning works using what is called \acp{NN}.

\acfp{NN} are a network of several layers of artificial neurons. Figure \ref{fig:snn}, shows a simple representation of a \ac{NN}, the artificial neurons are the represented by the colored circles. On figure \ref{fig:snn} each arrow represent a synapse.

\begin{figure}[h!]
  \centering
  \includesvg[height=8cm]{NN_explained.svg}
  \caption{Simple \acl{NN}}
  \label{fig:snn}
\end{figure}



They are several layers to a \ac{NN} :
\begin{itemize}
  \item Input layer : This layer is simply the different inputs.
  \item Hidden layer : This layer can be (and usually is) wider than the one in figure \ref{fig:snn}, it is there to add more layers and thus more precision to the result
  \item Output layer : This layer is where you can find the result from the \ac{NN}.
\end{itemize}

In a computational \ac{NN}, synapses are represented by weights. Those weights are to be multiplied by the last neuron and then added to each other to produce the next stage. Using the names defined in figure \ref{fig:snn}, the equation linking each layer is the following matrix multiplication :
\begin{equation}
  \begin{bmatrix}
    H_1\\ H_2\\ H_3\\ H_4\\
  \end{bmatrix}
  =
  \begin{bmatrix}
    W_{1,1} & W_{1,2} & W_{1,3}\\
    W_{2,1} & W_{2,2} & W_{2,3}\\
    W_{3,1} & W_{3,2} & W_{3,3}\\
    W_{4,1} & W_{4,2} & W_{4,3}\\
  \end{bmatrix}
  \cdot
  \begin{bmatrix}
    I_1\\ I_2\\ I_3\\
  \end{bmatrix}
\end{equation}

And it works the same way to go to the next layer.


Those matrix multiplication are called \ac{VMM} because it is the result of the multiplication of a vector and a matrix, thus giving us another vector.

Analog computation enables those same calculations almost instantly and being far more power-efficient. This can be done using the physical properties of electrical devices.
We want to reproduce one of those neural network using electrical circuits to perform the same computation that a computer would in much faster manner.
Using \hyperref[subsec:memristors]{memristors} as weights. This removes the need to copy the weights from the main memory
