\section{\acs{RNN}}\label{sec:rnn}

\acp{RNN} are, as the name suggests, a type of \ac{NN} using recurrent connections. They are a \ac{NN} with at least one cycle within the structure, where outputs of the previous run is used the next one. Those feedback connections are what differenciates it from feedforward \ac{NN}.

This type of \ac{NN} is used when dealing with an unknown amount of inputs. Especially useful when treating time series \cite{rnn}. Example of \acp{RNN} uses are speech recognition, automatic language translation \cite{gru} and shape recognition, especially for handwriting recognition.

They are trained the same way \ac{NN} are, measuring the error, backpropagte and adjust the weights accordingly.

\subsection{Simple \ac{RNN}}

The simple \ac{RNN} works just like a \ac{tanh} activated feedforward \ac{NN} with a feedback connection.

\Cref{eq:srnn} shows the equation that the simple \ac{RNN} runs at every time step.

\begin{equation}\label{eq:srnn}
h_t=tanh([x_t,h_{t-1}]\cdot w + b)
  \end{equation}

  Where

  \subsection{Vanishing gradient problem}

  The Vanishing gradient problem is a problem that comes when dealing with time depedent data \cite{vanishGrad}. When big amount of time depedent data is fed to the \ac{RNN}, the weights can't be updated properly. The older the data, the lower it will impact how much the weight must change. Rendering the old input data almost useless. Simple \acp{RNN} must be used with relatively short time series.

  Some \acp{RNN} were designed around this issue. This is the case of the \ac{LSTM} and \ac{GRU} which were created with internal mechanisms to regulate the flow of information and gradients.
