\section{\acs{LSTM}}\label{sec:lstm}
\acfp{LSTM} are a type of \ac{RNN} used to analyze sequence of data. They are capable of predict data based of the previous results. \acp{LSTM} are part of \acp{RNN}. \acp{RNN} are different than feedfoward \acl{NN} because of their feedback connections. Meaning that the results from the last time step have an impact on the next time step.

The first \ac{LSTM} was invented in 1997 by Hochreiter and Schmidhuber \cite{firstLSTM}. \acp{LSTM} changed a lot through the years to become what they are now.

\acp{LSTM} was created to fix the vanishing gradient problem. \acp{LSTM} alleviate this issue by adding a cell state, this state gives it the ability to choose what input is important and which one isn't, thus giving it a long term memory. This ability gave the uncommon name of \acl{LSTM} as it has both long and short term memory. This what gives \acp{LSTM} their use for sequence data. They can analyze the data and keep the information from the last time step to make a better decision afterwards. The most comprehensible example is considering a sentence. (TODO : find example)

An \ac{LSTM} is more complicated than just a simple feedforward \acl{NN}, they have several gates, which are all technically a \ac{NN} themselves. There is also a cell state which job is to hold a value for the next step.

\begin{figure}[H]
  \centering
  \includesvg[width=\textwidth,pretex=\large]{lstm/lstmCell.svg}
  \label{fig:lstmCell}
  \caption{\acs{LSTM} cell}
\end{figure}

\Cref{fig:lstmCell} shows the complexity of the \ac{LSTM} architecture. In an \ac{LSTM}, each gate is a different \ac{NN} and then activated with either a \ac{tanh} or a sigmoid activation function. Each input to the cell is a vector.
Those vector are of varying sizes depending on several factors. For example, both $h_t$ and $c_t$ are of the same size as the number of hidden state (sometimes refered to as cell state) for any $t\geq 0$.
The input vector ($x_t$) is of size of the sample we want to feed for each time step.

\subsection{Equations}

The equations of an LSTM are quite unusual.
Let's start with the more classic gate equations. They are the ones that behave like the more classic \ac{NN}.
The input (\cref{eq:inputG}), forget (\cref{eq:forgetG}) and ouput (\cref{eq:outputG}) gates are described below.


\begin{equation}\label{eq:inputG}
  \overrightarrow{i_t}=\sigma ([\overrightarrow{x_{t_1}},\overrightarrow{h_{t-1}}]\cdot w_i + \overrightarrow{b_i})
\end{equation}
\begin{equation}\label{eq:forgetG}
  \overrightarrow{f_t}=\sigma ([\overrightarrow{x_{t_1}},\overrightarrow{h_{t-1}}]\cdot w_f + \overrightarrow{b_f})
\end{equation}
\begin{equation}\label{eq:outputG}
  \overrightarrow{o_t}=\sigma ([\overrightarrow{x_{t_1}},\overrightarrow{h_{t-1}}]\cdot w_o + \overrightarrow{b_o})
\end{equation}

Where ($w_i$,$\overrightarrow{b_i}$), ($w_f$,$\overrightarrow{b_f}$) and ($w_o$,$\overrightarrow{b_o}$) are the pair of weights matrixes and biases vectors for the input, forget and ouput gates respectively. $\overrightarrow{x_t}$ is the input vector and $\overrightarrow{h_t}$ is the hidden state vector.

For the next equation describes the candidate cell state (\cref{eq:candCell}), that will next become the cell state (\cref{eq:cellS}).

\begin{equation}\label{eq:candCell}
  \overrightarrow{\tilde{c}_t}=tanh([\overrightarrow{x_{t_1}},\overrightarrow{h_{t-1}}]\cdot w_c+ \overrightarrow{b_c})
\end{equation}
\begin{equation}\label{eq:cellS}
  \overrightarrow{c_t}=\overrightarrow{f_t}\odot \overrightarrow{c_{t-1}} + \overrightarrow{i_t} \odot \overrightarrow{\tilde{c}_t}
\end{equation}

Where $w_c$ and $\overrightarrow{b_c}$ are the weights matrix and bias vector for the candidate cell state.

The final step of the \ac{LSTM} is to compute the hidden state (\cref{eq:hiddenS}).
\begin{equation}\label{eq:hiddenS}
  \overrightarrow{h_t}=\overrightarrow{o_t}\odot tanh(\overrightarrow{c_t})
\end{equation}

We set $x_1$ as the first input and define $\overrightarrow{h_0}$ as a zero only vector.

\subsection{Usage}

Using \ac{LSTM} can be a bit tricky. Due to it's sequential nature, it takes several time steps. Every hidden state ($h_t$) is passed to the next time step as an input. This hidden state can be used to compute an output at any time step $t$.

\begin{figure}[H]
  \centering
  \includesvg[width=\textwidth]{lstm/lstmUse}
  \caption{Unfolded \acs{LSTM}, legend in \cref{leg:cells}}
  \label{fig:lstmUse}
\end{figure}

\subsection{Variants}

\acp{LSTM} come in reality in a few different flavor. Usually, when \acp{LSTM} are mentionned, the version used is the \ac{NP} version. This is the most common version as those are the equations described on the wikipedia page \cite{wikiLSTM} and used in librairies like Keras \cite{Keras} or pyTorch \cite{PyTorch}. In fact, there are total of 8 other variations of \acp{LSTM} \cite{nbLSTM}.
The difference between each of them varies from one to the other, but they can all be described with new equations.

\subsubsection{Vanilla \ac{LSTM}}
This variant of the \ac{LSTM} was origanally the only \ac{LSTM} network, hence it's name. It differs from the classic \ac{LSTM} with it's use of peephole weights, hence the name of the classic \ac{LSTM} being \acl{NP}. The Vanilla \ac{LSTM} is thus defined by those equations \cite{vanillaLSTM, nbLSTM} :

\begin{equation}\label{eq:inputGVanilla}
  \overrightarrow{i_t}=\sigma ([\overrightarrow{x_{t_1}},\overrightarrow{h_{t-1}}]\cdot w_i + \overrightarrow{b_i} +\overrightarrow{c_{t-1}}\odot \overrightarrow{p_f})
\end{equation}
\begin{equation}\label{eq:forgetGVanilla}
  \overrightarrow{f_t}=\sigma ([\overrightarrow{x_{t_1}},\overrightarrow{h_{t-1}}]\cdot w_f + \overrightarrow{b_f}+\overrightarrow{c_{t-1}}\odot \overrightarrow{p_i})
\end{equation}
\begin{equation}\label{eq:ouputGVanilla}
  \overrightarrow{o_t}=\sigma ([\overrightarrow{x_{t_1}},\overrightarrow{h_{t-1}}]\cdot w_o + \overrightarrow{b_o}+\overrightarrow{c_{t}}\odot \overrightarrow{p_o})
\end{equation}

With $\overrightarrow{p_f}$, $\overrightarrow{p_i}$ and $\overrightarrow{p_o}$ being the peephole weights vectors. Their size is the same as the size of the hidden state vector ($\overrightarrow{h_t}$). $\odot$ is the pointwise multiplication operator.

The equations that haven't been rewritten simply stay the same.

\subsubsection{\acf{FGR} \ac{LSTM}}
This variant of the \ac{LSTM} is the most complex of all. This is due to the amount of feedback connections. This is basically a more complex version of the Vanilla \ac{LSTM}. The modified equations are :

\begin{equation}\label{eq:inputGFGR}
  \overrightarrow{i_t}=\sigma ([\overrightarrow{x_{t_1}},\overrightarrow{h_{t-1}}]\cdot w_i  + \overrightarrow{i_{t-1}}\cdot R_{ii} + \overrightarrow{f_{t-1}}\cdot R_{if} + \overrightarrow{o_{t-1}}\cdot R_{io} + \overrightarrow{b_i} +\overrightarrow{c_{t-1}}\odot \overrightarrow{p_i})
\end{equation}
\begin{equation}\label{eq:forgetGFGR}
  \overrightarrow{f_t}=\sigma ([\overrightarrow{x_{t_1}},\overrightarrow{h_{t-1}}]\cdot w_f  + \overrightarrow{i_{t-1}}\cdot R_{fi} + \overrightarrow{f_{t-1}}\cdot R_{ff} + \overrightarrow{o_{t-1}}\cdot R_{fo} + \overrightarrow{b_f} +\overrightarrow{c_{t-1}}\odot \overrightarrow{p_f})
\end{equation}
\begin{equation}\label{eq:ouputGFGR}
  \overrightarrow{o_t}=\sigma ([\overrightarrow{x_{t_1}},\overrightarrow{h_{t-1}}]\cdot w_o + \overrightarrow{i_{t-1}}\cdot R_{oi} + \overrightarrow{f_{t-1}}\cdot R_{of} + \overrightarrow{o_{t-1}}\cdot R_{oo} + \overrightarrow{b_o} +\overrightarrow{c_{t-1}}\odot \overrightarrow{p_o})
\end{equation}

With $R_{fi}$ being the weight matrix of the input feedback connection to compute the forget gate, and the same goes for the other weight matrices.

Once again, the equations that haven't been rewritten are unchanged.

\subsubsection{Other small variants}

\begin{itemize}
  \item Removing the activation function from the Vanilla \ac{LSTM} for :
    \begin{itemize}
      \item the candidate cell and thus becomes $ \tilde{c}_t=(w_c\cdot[x_{t-1},h_{t-1}] + b_c) $, this is called the \ac{NIAF}
      \item the output of the previous cell state and thus becomes $ h_t=o_t\cdot c_t $, and is called \ac{NOAF}
    \end{itemize}
  \item Using units instead of the respective gates :
    \begin{itemize}
      \item \ac{NIG} : $i_t=1$
      \item \ac{NOG} : $o_t=1$
      \item \ac{NFG} : $f_t=1$
    \end{itemize}
\end{itemize}
