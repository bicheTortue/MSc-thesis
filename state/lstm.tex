\section{\acs{LSTM}}\label{sec:lstm}
\acfp{LSTM} are a type of \ac{RNN} used to analyze sequences of data. They are capable of predicting data based on previous data points.

The first \ac{LSTM} was invented in 1997 by Hochreiter and Schmidhuber \cite{firstLSTM}. \acp{LSTM} changed a lot through the years to become what they are now.

\acp{LSTM} was created to fix the vanishing gradient problem. \acp{LSTM} alleviate this issue by adding a cell state, this state gives it the ability to choose what input is important and which one is not, thus giving it a long term memory. This ability gave the uncommon name of \acl{LSTM} as it has both long and short term memory. This is what makes \acp{LSTM} adequate for sequence data. They can analyze the data and keep the information from the last time step to make a better decision afterwards. The most comprehensible example is considering a sentence. %TODO : find example)

An \ac{LSTM} is more complicated than just a simple feedforward \acl{NN}, it has several gates, which all compute a \ac{VMM}. There is also a cell state whose job is to hold a value for the next step.

\begin{figure}[H]
  \centering
  \includesvg[width=\textwidth,pretex=\large]{lstm/lstmCell.svg}
  \label{fig:lstmCell}
  \caption{\acs{LSTM} cell, adapted from \cite{wikiLSTM}}
\end{figure}

\Cref{fig:lstmCell} shows the complexity of the \ac{LSTM} architecture. In an \ac{LSTM}, each gate is a different \ac{NN} and then activated with either a \ac{tanh} or a sigmoid activation function. Each input to the cell is a vector.
Those vectors are of varying sizes depending on several factors. For example, both $h_t$ and $c_t$ are of the same size as the number of hidden states (sometimes referred to as cell state) for any $t\geq 0$.
The input vector ($x_t$) is of size of the sample we want to feed for each time step.

\subsection{Equations}

The equations of an LSTM are quite unusual.
Let's start with the more classic gate equations. They are the ones that behave like the more classic \ac{NN}.
The input (\cref{eq:inputG}), forget (\cref{eq:forgetG}) and output (\cref{eq:outputG}) gates are described below.


\begin{equation}\label{eq:inputG}
  \overrightarrow{i_t}=\sigma ([\overrightarrow{x_{t_1}},\overrightarrow{h_{t-1}}]\cdot w_i + \overrightarrow{b_i})
\end{equation}
\begin{equation}\label{eq:forgetG}
  \overrightarrow{f_t}=\sigma ([\overrightarrow{x_{t_1}},\overrightarrow{h_{t-1}}]\cdot w_f + \overrightarrow{b_f})
\end{equation}
\begin{equation}\label{eq:outputG}
  \overrightarrow{o_t}=\sigma ([\overrightarrow{x_{t_1}},\overrightarrow{h_{t-1}}]\cdot w_o + \overrightarrow{b_o})
\end{equation}

Where ($w_i$,$\overrightarrow{b_i}$), ($w_f$,$\overrightarrow{b_f}$) and ($w_o$,$\overrightarrow{b_o}$) are the pair of weights matrixes and bias vectors for the input, forget and output gates respectively. $\overrightarrow{x_t}$ is the input vector and $\overrightarrow{h_t}$ is the hidden state vector.

The next equation describes the candidate cell state (\cref{eq:candCell}), that will next be used to compute the cell state (\cref{eq:cellS}).

\begin{equation}\label{eq:candCell}
  \overrightarrow{\tilde{c}_t}=tanh([\overrightarrow{x_{t_1}},\overrightarrow{h_{t-1}}]\cdot w_c+ \overrightarrow{b_c})
\end{equation}
\begin{equation}\label{eq:cellS}
  \overrightarrow{c_t}=\overrightarrow{f_t}\odot \overrightarrow{c_{t-1}} + \overrightarrow{i_t} \odot \overrightarrow{\tilde{c}_t}
\end{equation}

Where $w_c$ and $\overrightarrow{b_c}$ are the weights matrix and bias vector for the candidate cell state.

The final step of the \ac{LSTM} is to compute the hidden state (\cref{eq:hiddenS}).
\begin{equation}\label{eq:hiddenS}
  \overrightarrow{h_t}=\overrightarrow{o_t}\odot tanh(\overrightarrow{c_t})
\end{equation}

We set $x_1$ as the first input and define $\overrightarrow{h_0}$ as a zero only vector.

\subsection{Usage}

Using \ac{LSTM} can be a bit tricky. Due to its sequential nature, it takes several time steps. Every hidden state ($h_t$) is passed to the next time step as an input. This hidden state can be used to compute an output at any time step $t$. \Cref{fig:lstmUse} shows a visual representation of an \ac{LSTM} going from the current time step to the following time step.

\begin{figure}[H]
  \centering
  \includesvg[width=\textwidth]{lstm/lstmUse}
  \caption{Unfolded \acs{LSTM}, legend in \cref{leg:cells}}
  \label{fig:lstmUse}
\end{figure}

\subsection{Variants}

\acp{LSTM} come in a few different flavors of implementations. Usually, when \acp{LSTM} are mentionned, the version used is the \ac{NP} version. This is the most common version as those are the equations described on the wikipedia page \cite{wikiLSTM} and used in libraries like Keras \cite{Keras} or PyTorch \cite{PyTorch}. In fact, there are at least 8 other variations of \acp{LSTM} \cite{nbLSTM}.
The difference between them varies from one to the other, some of them are detailled next.

\subsubsection{Vanilla \ac{LSTM}}
This variant of the \ac{LSTM} was originally the only \ac{LSTM} network, hence its name. It differs from the classic \ac{LSTM} with its use of peephole weights, hence the name of the classic \ac{LSTM} being \acl{NP}. The Vanilla \ac{LSTM} is thus defined by the following equations \cite{vanillaLSTM, nbLSTM} :

\begin{equation}\label{eq:inputGVanilla}
  \overrightarrow{i_t}=\sigma ([\overrightarrow{x_{t_1}},\overrightarrow{h_{t-1}}]\cdot w_i + \overrightarrow{b_i} +\overrightarrow{c_{t-1}}\odot \overrightarrow{p_f})
\end{equation}
\begin{equation}\label{eq:forgetGVanilla}
  \overrightarrow{f_t}=\sigma ([\overrightarrow{x_{t_1}},\overrightarrow{h_{t-1}}]\cdot w_f + \overrightarrow{b_f}+\overrightarrow{c_{t-1}}\odot \overrightarrow{p_i})
\end{equation}
\begin{equation}\label{eq:ouputGVanilla}
  \overrightarrow{o_t}=\sigma ([\overrightarrow{x_{t_1}},\overrightarrow{h_{t-1}}]\cdot w_o + \overrightarrow{b_o}+\overrightarrow{c_{t}}\odot \overrightarrow{p_o})
\end{equation}

With $\overrightarrow{p_f}$, $\overrightarrow{p_i}$ and $\overrightarrow{p_o}$ being the peephole weights vectors. Their size is the same as the size of the hidden state vector ($\overrightarrow{h_t}$). $\odot$ is the pointwise multiplication operator.

The equations that haven't been rewritten simply stay the same.

\subsubsection{\acf{FGR} \ac{LSTM}}
This variant of the \ac{LSTM} is the most complex of all. This is due to the amount of feedback connections. This is basically a more complex version of the Vanilla \ac{LSTM}. The modified equations are :

\begin{equation}\label{eq:inputGFGR}
  \overrightarrow{i_t}=\sigma ([\overrightarrow{x_{t_1}},\overrightarrow{h_{t-1}}]\cdot w_i  + \overrightarrow{i_{t-1}}\cdot R_{ii} + \overrightarrow{f_{t-1}}\cdot R_{if} + \overrightarrow{o_{t-1}}\cdot R_{io} + \overrightarrow{b_i} +\overrightarrow{c_{t-1}}\odot \overrightarrow{p_i})
\end{equation}
\begin{equation}\label{eq:forgetGFGR}
  \overrightarrow{f_t}=\sigma ([\overrightarrow{x_{t_1}},\overrightarrow{h_{t-1}}]\cdot w_f  + \overrightarrow{i_{t-1}}\cdot R_{fi} + \overrightarrow{f_{t-1}}\cdot R_{ff} + \overrightarrow{o_{t-1}}\cdot R_{fo} + \overrightarrow{b_f} +\overrightarrow{c_{t-1}}\odot \overrightarrow{p_f})
\end{equation}
\begin{equation}\label{eq:ouputGFGR}
  \overrightarrow{o_t}=\sigma ([\overrightarrow{x_{t_1}},\overrightarrow{h_{t-1}}]\cdot w_o + \overrightarrow{i_{t-1}}\cdot R_{oi} + \overrightarrow{f_{t-1}}\cdot R_{of} + \overrightarrow{o_{t-1}}\cdot R_{oo} + \overrightarrow{b_o} +\overrightarrow{c_{t-1}}\odot \overrightarrow{p_o})
\end{equation}

With $R_{fi}$ being the weight matrix of the input feedback connection to compute the forget gate, and the same goes for the other weight matrixes.

Once again, the equations that haven't been rewritten are unchanged.

\subsubsection{Other small variants}

\begin{itemize}
  \item Removing the activation function from the Vanilla \ac{LSTM} for :
    \begin{itemize}
      \item the candidate cell and thus becomes $ \overrightarrow{\tilde{c}_t}=(w_c\cdot[\overrightarrow{x_{t-1}},\overrightarrow{h_{t-1}}] + \overrightarrow{b_c}) $, this is called the \ac{NIAF}
      \item the output of the previous cell state and thus becomes $ \overrightarrow{h_t}=\overrightarrow{o_t}\odot \overrightarrow{c_t} $, and is called \ac{NOAF}
    \end{itemize}
  \item Using units instead of the respective gates :
    \begin{itemize}
      \item \ac{NIG} : $\overrightarrow{i_t}=1$
      \item \ac{NOG} : $\overrightarrow{o_t}=1$
      \item \ac{NFG} : $\overrightarrow{f_t}=1$
    \end{itemize}
\end{itemize}
