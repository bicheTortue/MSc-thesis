\section{\aclp{NN}}\label{sec:nn}

\acfp{NN} are a set of units known as neurons. Those neurons are linked to each other with arcs known as synapses, thoses synapses each have a weight associated to them. The set of neurons interconnected with their synapses is what is called a \acl{NN}.
\Cref{fig:snn}, shows a simple representation of a \ac{NN}, the artificial neurons are the represented by the colored circles. On \cref{fig:snn} each arrow represent a synapse.

\begin{figure}[h!]
  \centering
  \includesvg[height=8cm]{NN_explained.svg}
  \caption{Simple \acl{NN}}
  \label{fig:snn}
\end{figure}

\acp{NN} contains several layers :

\begin{itemize}
  \item Input layer : This layer is simply the different inputs.
  \item Hidden layer : This layer can be (and usually is) wider than the one in figure \ref{fig:snn}. This is the layer that can be modified the most, by adding layers or increasing the amount of neurons in a layer.
  \item Output layer : This layer is where you can find the result from the \ac{NN}.
\end{itemize}

The weights of the synapses have to be multiplied with the previous neuron and then added to each other to produce the next stage. Using the names defined in figure \ref{fig:snn}, the output is linked to the input by \cref{eq:nnHid,eq:nnOut}. First, the hidden layers' neurons need to be computed (\cref{eq:nnHid}).

\begin{equation}\label{eq:nnHid}
  \begin{bmatrix}
    H_1\\ H_2\\ H_3\\ H_4\\
  \end{bmatrix}
  =
  \begin{bmatrix}
    w_h_{1,1} & w_h_{1,2} & w_h_{1,3}\\
    w_h_{2,1} & w_h_{2,2} & w_h_{2,3}\\
    w_h_{3,1} & w_h_{3,2} & w_h_{3,3}\\
    w_h_{4,1} & w_h_{4,2} & w_h_{4,3}\\
  \end{bmatrix}
  \cdot
  \begin{bmatrix}
    I_1\\ I_2\\ I_3\\
  \end{bmatrix}
\end{equation}

Similarly the output is computed like in \cref{eq:nnOut}

\begin{equation}\label{eq:nnOut}
  \begin{bmatrix}
    O_1\\ O_2
  \end{bmatrix}
  =
  \begin{bmatrix}
    w_o_{1,1} & w_o_{1,2} & w_o_{1,3} & w_o_{1,4}\\
    w_o_{2,1} & w_o_{2,2} & w_o_{2,3} & w_o_{2,4}\\
  \end{bmatrix}
  \cdot
  \begin{bmatrix}
    H_1\\ H_2\\ H_3\\ H_4\\
  \end{bmatrix}
\end{equation}

Those matrix multiplication are called \ac{VMM} because it is the result of the multiplication of a vector and a matrix, thus giving us another vector.

The model presented in \cref{fig:snn} is modular and can be scaled up as much as required. It is generally accepted that the more neurons a \ac{NN} has, the more complex the problems  it can solve can be.

\subsection{Training weights}

The weights values are obtained through training. In supervised learning, in order train the weights it is required to have a dataset of inputs/outputs values that are correct. The outputs are the target values, the values that the \ac{NN} needs to compute when being executed. The training starts once the weights have been initialized (usually randomly picked values). The training is an iteravtive process, each iteration being called an epoch. Each of those epoch usally consists of the following steps :

\begin{enumerate}
  \item Run the \ac{NN} to get an output vector.\label{step:restart}
  \item Measure the error (known as loss) by comparing the current prediction with the targeted output using the chosen error algorithm (\ac{RMSE}, \ac{MSE}, etc).
  \item Run the backpropagation algorithm to change each individual weight.
\end{enumerate}

The number of epochs required depends on the complexity of the problem the \ac{NN} is solving.

Once trained, the \ac{NN} can be tested by feeding the \ac{NN} input data it has not seen before.
