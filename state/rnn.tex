\section{\acl{RNN}}\label{sec:rnn}

\acp{RNN} are, as the name suggests, a type of \ac{NN} using recurrent connections. They are a \ac{NN} with at least one cycle within the structure, outputs of previous time step are used as input for the next time step, these outputs are generally known are hidden states. Those feedback connections are the main difference from feedforward \ac{NN}.

This type of \ac{NN} is used when dealing with an unknown amount of inputs. Especially useful when treating time series \cite{rnn}. Example of \acp{RNN} uses are speech recognition, automatic language translation \cite{gru} and shape recognition, especially for handwriting recognition.

Traditionnal \acp{RNN} have the ability to model sequential events by propagating through time, for example forward and backward propagation. This is achieved by connecting these sequential events with the hidden state like in \cref{eq:rnn}.

\begin{equation}\label{eq:rnn}
  \symvh_t=f(\symvx_t,\symvh_{t-1})
\end{equation}

The hidden state ($\symvh_t$) carries all the past informations for the next time step. It also serves as the output of the \ac{RNN}.

They are trained the same way \ac{NN} are, measuring the error, backpropagating and adjusting the weights accordingly.

\subsection{Simple \acl{RNN}}

The simple \ac{RNN} works just like a \ac{tanh} activated feedforward \ac{NN} with a feedback connection.

\begin{figure}[H]
  \centering
  \begin{minipage}{\columnwidth}
    \subfloat[\acs{RNN} cell\label{fig:rnnCell}]{\includesvg[width=\textwidth,pretex=\large]{rnn/rnnCell.svg}}
  \end{minipage}
  \begin{minipage}{\columnwidth}
    \subfloat[Legend\label{leg:cells}]{\includesvg[width=\textwidth,pretex=\large]{cellsLegend.svg}}
  \end{minipage}
  \caption{}
\end{figure}

\Cref{eq:srnn} shows the equation that the simple \ac{RNN} runs at every time step.

\begin{equation}\label{eq:srnn}
  \symvh_t=tanh([\symvx_t,\symvh_{t-1}]\cdot \symmw + \symvb)
\end{equation}

Where $t\in\mathbb{N}^*$ is the time index, \symmw the weight matrix, \symvb the bias vector, \symvx the input vector and \symvh the hidden state of the \ac{RNN}.

\subsection{Vanishing gradient problem}

The Vanishing gradient problem is a problem that comes when dealing with time dependent data \cite{vanishGrad}. When a big amount of time dependent data is fed to the \ac{RNN}, the weights cannot be updated properly. The older the data, the lower it will impact how much the weight must change. Rendering the old input data almost useless. That is the reasom why, simple \acp{RNN} must be used with relatively short time series.

Some \acp{RNN} were designed to tackle this issue. This is the case of the \ac{LSTM} and \ac{GRU} which were created with internal mechanisms to regulate the flow of information and gradients.
