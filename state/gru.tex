\section{\acs{GRU}}\label{sec:gru}
The \acf{GRU} is another type of \ac{RNN}. It is also known to reduce the effect of the vanishing gradient problem. It was first introduced to improve translation techniques \cite{gru}.

The \ac{GRU} is very often compared to the \ac{LSTM}, it is sometimes assimilated as a type of \ac{LSTM} \cite{nbLSTM}. Their performance was found to be very similar in most situations, making those two types of \acp{RNN} coexistant in the modern machine learning world.

There are two versions of the \ac{GRU}, both are found on the internet, they are known as the encoder and decoder version \cite{gru}. They were originally designed to encode the message to translate and then decode in the translation. PyTorch only supports the decoder version \cite{gruPyTorch}, while the keras library supports both \cite{gruKeras} chosen by changing an argument.

\subsection{Encoder \ac{GRU}}

The encoder \ac{GRU} is the version of the \ac{GRU} most widely described on the internet. It contains an update gate (\cref{eq:updateG}), a reset gate (\cref{eq:resetG}), a candidate activation gate (\cref{eq:candActivG}). The hidden state is then computed (\cref{eq:gruHidG}) using the previous results.

\begin{equation}\label{eq:updateG}
  \overrightarrow{z_t}=\sigma ([\overrightarrow{x_t},\overrightarrow{h_{t-1}}] \cdot w_z + \overrightarrow{b_z})
\end{equation}
\begin{equation}\label{eq:resetG}
  \overrightarrow{r_t}=\sigma ([\overrightarrow{x_t},\overrightarrow{h_{t-1}}] \cdot w_r + \overrightarrow{b_r})
\end{equation}
\begin{equation}\label{eq:candActivG}
  \overrightarrow{\hat{h_t}}=tanh(\overrightarrow{x_t}\cdot w_{hx}+(\overrightarrow{r_t}\odot\overrightarrow{h_{t-1}}) \cdot w_{hh} + \overrightarrow{b_h})
\end{equation}
\begin{equation}\label{eq:gruHidG}
  \overrightarrow{h_t}=(\overrightarrow{1}-\overrightarrow{z_t})\odot \overrightarrow{h_{t-1}} + \overrightarrow{z_t}\odot \overrightarrow{\hat{h_t}}
\end{equation}

Where ($w_z$,$\overrightarrow{b_z}$), ($w_r$,$\overrightarrow{b_r}$),($w_{hx}$,$w_{hh}$,$\overrightarrow{b_h}$) are the weights matrixes and bias vectors for the update, reset and candidate activation gates respectively.

\begin{figure}[H]
  \centering
  \includesvg[width=\textwidth,pretex=\large]{gru/encoderCell.svg}
  \label{fig:lstmCell}
  \caption{Encoder \acs{GRU} cell, legend in \cref{leg:cells}}
\end{figure}

Sometimes the \cref{eq:gruHidG} can be found in another form (\cref{eq:gruHidG1})\cite{gruPyTorch}. This, however, has no impact on the final results, it means the weights are going to be trained differently for the update gate.

\begin{equation}\label{eq:gruHidG1}
  \overrightarrow{h_t}=\overrightarrow{z_t}\odot \overrightarrow{h_{t-1}} + (\overrightarrow{1}-\overrightarrow{z_t})\odot \overrightarrow{\hat{h_t}}
\end{equation}

\subsection{Decoder \ac{GRU}}

The decoder \ac{GRU}, while being less described, is the version used in pyTorch, which is getting very popular.

The candidate activation gate (\cref{eq:candActivG1}) is the only difference from the encoder \ac{GRU}.

\begin{equation}\label{eq:candActivG1}
  \overrightarrow{\hat{h_t}}=tanh(\overrightarrow{x_t}\cdot w_{hx}+ \overrightarrow{b_{hx}}+\overrightarrow{r_t}\odot[\overrightarrow{h_{t-1}} \cdot w_{hh} + \overrightarrow{b_{hh}}])
\end{equation}

\begin{figure}[H]
  \centering
  \includesvg[width=\textwidth,pretex=\large]{gru/decoderCell.svg}
  \label{fig:lstmCell}
  \caption{Decoder \acs{GRU} cell, legend in \cref{leg:cells}}
\end{figure}

\subsection{Similarities with \ac{LSTM}}

While the \ac{GRU} is its own type of \ac{RNN}, it has been compared and assimilated to an \ac{LSTM} \cite{nbLSTM}. Indeed, the subtype of \ac{LSTM} it has been associated with is called the \ac{CIFG} \ac{LSTM}. If the input and forget gates are combined into one ($\overrightarrow{f_t}=\overrightarrow{1}-\overrightarrow{i_t}$), they can be assimilated as the update gate of the \ac{GRU}. The reset gate would then be the \ac{LSTM}'s output gate.

\Cref{eq:lstmGru0,eq:lstmGru1,eq:lstmGru2,eq:lstmGru3,eq:lstmGru4} are the \ac{GRU}'s equations with the \ac{LSTM}'s names.

\begin{equation}\label{eq:lstmGru0}
  \overrightarrow{f_t}=\sigma ([\overrightarrow{x_t},\overrightarrow{h_{t-1}}] \cdot w_f + \overrightarrow{b_f})
\end{equation}
\begin{equation}\label{eq:lstmGru1}
  \overrightarrow{o_t}=\sigma ([\overrightarrow{x_t},\overrightarrow{h_{t-1}}] \cdot w_o + \overrightarrow{b_o})
\end{equation}
\begin{equation}\label{eq:lstmGru2}
  \overrightarrow{i_t}=\overrightarrow{1}-\overrightarrow{f_t}
\end{equation}
\begin{equation}\label{eq:lstmGru3}
  \overrightarrow{\tilde{c_t}}=tanh(\overrightarrow{x_t}\cdot w_{cx}+(\overrightarrow{o_t}\odot\overrightarrow{h_{t-1}}) \cdot w_{ch} + \overrightarrow{b_c})
\end{equation}
\begin{equation}\label{eq:lstmGru4}
  \overrightarrow{h_t}=(\overrightarrow{i_t})\odot \overrightarrow{h_{t-1}} + \overrightarrow{f_t}\odot \overrightarrow{\tilde{h_t}}
\end{equation}

Differences between the two \acp{RNN} are :
\begin{itemize}
  \item The \ac{GRU} has no output activation function.
  \item The \ac{GRU} has combined hidden state and cell state.
  \item The \ac{GRU}'s candidate hidden state is point wise multiplied with the reset gate before passing into the dense layer.
\end{itemize}
