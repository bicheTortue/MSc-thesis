\section{\acl{GRU}}\label{sec:gru}
The \acf{GRU} is another type of \ac{RNN}. It is also known to reduce the effect of the vanishing gradient problem. It was first introduced to improve translation techniques \cite{gru}.

The \ac{GRU} is very often compared to the \ac{LSTM}, being sometimes assimilated as a type of \ac{LSTM} \cite{nbLSTM}. Their performance was found to be very similar in most situations \cite{gruVSlstm}, making those two types of \acp{RNN} coexistant in the modern machine learning world.

There are two versions of the \ac{GRU}, both are found on the internet, they are known as the encoder and decoder version \cite{gru}. They were originally designed to encode the message to translate and then decode in the translation. PyTorch only supports the decoder version \cite{gruPyTorch}, while the Keras library supports both \cite{gruKeras} chosen by changing an argument.

\subsection{Encoder \ac{GRU}}

The encoder \ac{GRU} is the version of the \ac{GRU} most widely described on the internet. It contains an update gate (\cref{eq:updateG}), a reset gate (\cref{eq:resetG}), a candidate activation gate (\cref{eq:candActivG}). The hidden state is then computed (\cref{eq:gruHidG}) using the previous results.

\begin{equation}\label{eq:updateG}
  \symvz_t=\sigma ([\symvx_t,\symvh_{t-1}] \cdot \symmw_z + \symvb_z)
\end{equation}
\begin{equation}\label{eq:resetG}
  \symvr_t=\sigma ([\symvx_t,\symvh_{t-1}] \cdot \symmw_r + \symvb_r)
\end{equation}
\begin{equation}\label{eq:candActivG}
  \symvhh_t=tanh(\symvx_t\cdot \symmw_{hx}+(\symvr_t\odot\symvh_{t-1}) \cdot \symvw_{hh} + \symvb_h)
\end{equation}
\begin{equation}\label{eq:gruHidG}
  \symvh_t=(\vunit-\symvz_t)\odot \symvh_{t-1} + \symvz_t\odot \symvhh_t
\end{equation}

Where ($\symmw_z$,$\symvb_z$), ($\symmw_r$,$\symvb_r$),($\symmw_{hx}$,$\symmw_{hh}$,$\symvb_h$) are the weights matrixes and bias vectors for the update, reset and candidate activation gates respectively.

A visual representation of the encoder \ac{GRU} cell is available in \cref{fig:encoderGruCell}.

\begin{figure}[H]
  \centering
  \includesvg[width=\textwidth,pretex=\large]{gru/encoderCell.svg}
  \caption{Encoder \acs{GRU} cell, legend in \cref{leg:cells}}
  \label{fig:encoderGruCell}
\end{figure}

Sometimes the \cref{eq:gruHidG} can be found in another form (\cref{eq:gruHidG1})\cite{gruPyTorch}. This, however, has no impact on the final results, it means the weights are going to be trained differently for the update gate.

\begin{equation}\label{eq:gruHidG1}
  \symvh_t=\symvz_t\odot \symvh_{t-1} + (\vunit-\symvz_t)\odot \symvh_t
\end{equation}

\subsection{Decoder \ac{GRU}}

The decoder \ac{GRU}, while being less described, is the version used in pyTorch, which is getting very popular.

The candidate activation gate (\cref{eq:candActivG1}) is the only difference from the encoder \ac{GRU}.

\begin{equation}\label{eq:candActivG1}
  \symvhh_t=tanh(\symvx_t\cdot \symmw_{hx}+ \symvb_{hx}+\symvr_t\odot[\symvh_{t-1} \cdot \symmw_{hh} + \symvb_{hh}])
\end{equation}

A visual representation of the decoder \ac{GRU} cell can be found in \cref{fig:decoderGruCell}.

\begin{figure}[H]
  \centering
  \includesvg[width=\textwidth,pretex=\large]{gru/decoderCell.svg}
  \caption{Decoder \acs{GRU} cell, legend in \cref{leg:cells}}
  \label{fig:decoderGruCell}
\end{figure}

\subsection{Similarities with \ac{LSTM}}

While the \ac{GRU} is its own type of \ac{RNN}, it has been compared and assimilated to an \ac{LSTM} \cite{nbLSTM}. Indeed, the subtype of \ac{LSTM} it has been associated with is called the \ac{CIFG} \ac{LSTM}. If the input and forget gates are combined into one ($\overrightarrow{f_t}=\overrightarrow{1}-\overrightarrow{i_t}$), they can be assimilated as the update gate of the \ac{GRU}. The reset gate would then be the \ac{LSTM}'s output gate.

\Cref{eq:lstmGru0,eq:lstmGru1,eq:lstmGru2,eq:lstmGru3,eq:lstmGru4} are the \ac{GRU}'s equations with the \ac{LSTM}'s names.

\begin{equation}\label{eq:lstmGru0}
  \symvf_t=\sigma ([\symvx_t,\symvh_{t-1}] \cdot \symmw_f + \symvb_f)
\end{equation}
\begin{equation}\label{eq:lstmGru1}
  \symvo_t=\sigma ([\symvx_t,\symvh_{t-1}] \cdot \symmw_o + \symvb_o)
\end{equation}
\begin{equation}\label{eq:lstmGru2}
  \symvi_t=\vunit-\symvf_t
\end{equation}
\begin{equation}\label{eq:lstmGru3}
  \symvct_t=tanh(\symvx_t\cdot \symmw_{cx}+(\symvo_t\odot\symvh_{t-1}) \cdot \symmw_{ch} + \symvb_c)
\end{equation}
\begin{equation}\label{eq:lstmGru4}
  \symvh_t=(\symvi_t)\odot \symvh_{t-1} + \symvf_t\odot \symvct_t
\end{equation}

Differences between the two \acp{RNN} are :
\begin{itemize}
  \item The \ac{GRU} has no output activation function.
  \item The \ac{GRU} has combined hidden state and cell state.
  \item The \ac{GRU}'s candidate hidden state is point wise multiplied with the reset gate before passing into the dense layer.
\end{itemize}
