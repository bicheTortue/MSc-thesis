\section{Airline dataset}
\label{sec:resAirline}

\subsection{Network configuration}

The layers used to solve this problem are listed below :

\begin{itemize}
  \item An \ac{LSTM} with four hidden states ($n_h=4$) and an input with feature size of one and two time steps.
  \item A Dense layer with an output size of one.
\end{itemize}

\Cref{fig:airlineModel} is a graphical representation of the model just described.

\begin{figure}[H]
  \centering
  \includesvg[width=\textwidth]{datasets/airlineModel}
  \caption{Model used to solve the airline passengers problem}
  \label{fig:airlineModel}
\end{figure}

The same network configuration using a \ac{GRU} layer instead of an \ac{LSTM} layer. The number of hidden states is the same.

The \ac{NN} was trained for $300$ epochs.

The timing of the flags can be quite hard to keep track of, for this reason all of them are going to be shown for the full duration of the system execution to get one output.

\Cref{tim:airline} shows the time diagram of the entire execution using the parameters discussed earlier.

\begin{figure}[H]
  \centering
  \begin{tikztimingtable}
    $e_0$       & x 2H 6L     L 2H 6L     L L x\\
    $e_1$       & x 2L 2H 4L  L 2L 2H 4L  L L x\\
    $e_2$       & x 4L 2H 2L  L 4L 2H 2L  L L x\\
    $e_3$       & x 6L 2H     L 6L 2H     L L x\\
    $e_{next}$  & x 8L        H 8L        H L x\\
    $e_{in}$    & x 8H        L 8H        L L x\\
    $e_{out}$   & x 8L        L 8L        L H x\\
    \extracode
    \tablerules
    \begin{pgfonlayer}{background}
      \vertlines[help lines]{0.55,8.55,9.55,17.55,18.55}
      %\vertlines[red]{1.6,5.6,15.6}
      %\vertlines[blue]{3.6,9.6,15.6}
    \end{pgfonlayer}
  \end{tikztimingtable}
  \caption{Flags time diagram for the airline problem with $n_s=4$}
  \label{tim:airline}
\end{figure}

\subsection{Digital results}
\label{subsec:digitalAirline}

The digital results of the \ac{NN} once trained are very important. They are the results that the analog system will have to reproduce. Furthermore, this part is used to measure the impact of the custom activation functions used for training (\cref{sec:af}).

\subsubsection{\acs{LSTM} predictions}

\begin{figure}[H]
  \centering
  \includesvg[width=\linewidth]{datasets/airlineDigital}
  \caption{Graph of the digital predictions for airline dataset. The dotted vertical line shows the limit of the data used for training and the one used for validation.}
  \label{graph:airlineDigital}
\end{figure}

\Cref{graph:airlineDigital} shows the results of the \ac{NN} next to the orinal data. The results are visually very close. The error is quantified using \ac{RMSE} function. All those curves error to each other are measured and displayed in \cref{tab:airlineDigital}.

\begin{table}[H]
  \centering
  \begin{tabular}{|c|c|c|c|}
    \hline
    \cellcolor[HTML]{808080}\acs{RMSE} & Target data & \specialcell{Digital prediction with\\default activation functions} & \specialcell{Digital prediction with\\analog activation functions}\\
    \hline
    Target data &\cellcolor[HTML]{202020} & $54.6$ & $52.3$\\
    \hline
    \specialcell{Digital prediction with\\default activation functions}  & $54.6$ & \cellcolor[HTML]{202020} & $78.6$\\
    \hline
    \specialcell{Digital prediction with\\analog activation functions} & $52.3$ & $78.6$ & \cellcolor[HTML]{202020}\\
    \hline
  \end{tabular}
  \caption{\acp{RMSE} of each curve to the others}
  \label{tab:airlineDigital}
\end{table}

Both predictions have similar \ac{RMSE} to the target curve. They have a \ac{RMSE} of $54.6$ for the \ac{LSTM} using the default activation functions (sigmoid and \ac{tanh} functions) and $52.3$ for the one using the analog activation functions (\cref{sec:af}). The error difference is so low (about two thousands passengers) that the difference may just come from the training and the initial values of the weights. They both approach the target curve pretty well. These error could be lowered by training for more epochs. This is not done because it diverges too much from the focus of the thesis.

The two predictions are very different from each other, the error rate between the two is of $78.6$, which is significantly higher than the \ac{RMSE} of the curves to the target curve.

Computing the \ac{NN} with or without the analog activation functions does not seem to affect the error of the predictions, experience has shown that the \ac{LSTM} using the analog activation functions seem to predict higher curves than the target and the \ac{LSTM} using the default activation functions tends to predict curves that are under the target, the reasons for such a difference are unknown. This is simply an observation after a few different runs.

\subsubsection{\acs{GRU} predictions}

Unlike for the predictions using an \ac{LSTM} layer, the weights were only trained using the analog activation functions. The effect of using the default activation functions is already shown with the \ac{LSTM} layer.

The graph of the resulting prediction is the one in \cref{graph:airlineDigitalGRU}, the error rate is also in the figure.

Interestingly, the results look much better when using the \ac{GRU} layer. The error rate of $36,6$ confirms that as it is almost half of what it was with the \ac{LSTM} layer (\cref{tab:airlineDigital}). That simply means that the \ac{GRU} layer is better at handling this problem, as was previously stated in \cref{sec:gru}.

\begin{figure}[H]
  \centering
  \includesvg[width=\linewidth]{datasets/airlineDigitalGRU}
  \caption{Graph of the digital predictions along with the \ac{RMSE} for the airline dataset using a \ac{GRU} layer. The dotted vertical line shows the limit of the data used for training and the one used for validation.}
  \label{graph:airlineDigitalGRU}
\end{figure}

\subsection{Analog results trained with default activation functions}\label{subsec:airlineAnalogNoC}

This subsection shows the importance of using the analog activation functions in the training phase of the \ac{NN}. The \ac{LSTM} layer being the only to have been trained using the default activation functions the only results presented here are using an \ac{LSTM} layer.

\begin{figure}[H]
  \centering
  \includesvg[width=\linewidth]{results/airline/noCustomAF}
  \caption{Analog predictions trained with the default activation functions.}
  \label{graph:airlineAnalogNoC}
\end{figure}

As expected, the analog predictions are very far off the target curve. Which, in this case, is not the target dataset but the digital prediction ran with the default activation functions. Indeed, the goal of the analog system is to reproduce as closely as possible the digital prediction.

\begin{table}[H]
  \centering
  \begin{tabular}{|c|c|c|c|}
    \hline
    \multirow{2}{*}{\acs{RMSE}} & \multicolumn{3}{c|}{Analog prediction}\\
    \cline{2-4}
    & $n_s=1$ & $n_s=2$ & $n_s=4$ \\
    \hline
    \specialcell{Digital prediction with\\default activation functions} & $141$ & $144$ & $140$\\
    \hline
  \end{tabular}
  \caption{\acp{RMSE} of each analog prediction to their associated digital prediction}
  \label{tab:airlineAnalogNoC}
\end{table}

As can be observed on \cref{graph:airlineAnalogNoC}, the analog predictions are not on par with what is expected at all. This is confirmed by the error rates found in \cref{tab:airlineAnalogNoC}, the error rates are not in an acceptable range at all. This error would be even higher to the target dataset as the digital prediction using the default activation functions is already under than the target curve.

However, the prediction do not seem to be completely off. The curves seem to follow the shape of its target, and by extension of the target data. This deserves a closer look.

\begin{figure}[H]
  \centering
  \includesvg[width=\linewidth]{results/airline/noCustomAFZoomed}
  \caption{Analog predictions trained with the default activation functions zoomed on the the analog predictions.}
  \label{graph:airlineAnalogNoCZoomed}
\end{figure}

As expected, in \cref{graph:airlineAnalogNoCZoomed} the trend of the curves is very similar to the original dataset (\cref{graph:airline}) and the predictions are very close to each others no matter the serial size used ($n_s$).

\begin{table}[H]
  \centering
  \begin{tabular}{|c|c|c|c|c|}
    \hline
    \multicolumn{2}{|c}{\multirow{2}{*}{\ac{RMSE}}} & \multicolumn{3}{|c|}{Analog prediction}\\
    \cline{3-5}
    \multicolumn{2}{|c}{} & \multicolumn{1}{|c|}{$n_s=1$} & $n_s=2$ & $n_s=4$ \\
    \hline
    \multirow{3}{*}{Analog prediction} & $n_s=1$ &\cellcolor[HTML]{202020} & $3.97$ & $5.05$\\
    \cline{2-5}
    & $n_s=2$  & $3.97$ & \cellcolor[HTML]{202020} & $4.78$\\
    \cline{2-5}
    & $n_s=4$ & $5.05$ & $4.78$ & \cellcolor[HTML]{202020}\\
    \hline
  \end{tabular}
  \caption{\acp{RMSE} of each analog prediction to the others depending on the serial size ($n_s$)}
  \label{tab:airlineAnalogNoCZoomed}
\end{table}

This similarity is further proven by the errors shown in \cref{tab:airlineAnalogNoCZoomed}. The erros are $3.97$, $4.78$ and $5.05$. Those error are all around four thousands passengers, which is negligible in our case. That shows a certain stability of the system when using different values of serial size ($n_s$). The error rate is also quite low because of the lower results obtained.

The predicted results generated by the analog system seem to be on a different scale while still following the overall aspect of the dataset. The down scale of those analog prediction would also mean that the errors of those predictions to each other is also scaled down.

\subsection{Analog results trained with analog activation functions}

\subsubsection{\ac{LSTM} predictions}

This part will show the result of the inference when the weights were trained using the analog activation functions.

\begin{figure}[H]
  \centering
  \includesvg[width=\linewidth]{results/airline/analog}
  \caption{Analog predictions trained with the analog activation functions.}
  \label{graph:airlineAnalog}
\end{figure}

As can be observed in \cref{graph:airlineAnalog}, the analog results are, in this case, way closer to the target values. The curves are very close to eahc other and are a definite improvement from the the ones trained using default activation functions.

\begin{table}[H]
  \centering
  \begin{tabular}{|c|c|c|c|}
    \hline
    \multirow{2}{*}{\acs{RMSE}} & \multicolumn{3}{|c|}{Analog prediction}\\
    \cline{2-4}
    & $n_s=1$ & $n_s=2$ & $n_s=4$ \\
    \hline
    \specialcell{Digital prediction with\\analog activation functions} & $28.4$ & $92.6$ & $68.5$\\
    \hline
  \end{tabular}
  \caption{\acp{RMSE} of each analog prediction to their associated digital prediction}
  \label{tab:airlineAnalog}
\end{table}

\Cref{tab:airlineAnalog} contains the errors of the analog predictions. It can be observed that the results the closest to the target are the ones generated using a serial size of one ($n_s=1$). Indeed, its error ($28.4$) is lower than the error from the digital results ($52.3$) to the original target values, almost half of it. The other ones are still very close to the targeted curve, but are still lower. The errors are higher when running the system in serial mode. This higher inaccuracy is probably due to the data staying for longer in the memory cells as the time steps get longer.

\begin{table}[H]
  \centering
  \begin{tabular}{|c|c|c|c|c|}
    \hline
    \multicolumn{2}{|c}{\multirow{2}{*}{\ac{RMSE}}} & \multicolumn{3}{|c|}{Analog prediction}\\
    \cline{3-5}
    \multicolumn{2}{|c}{} & \multicolumn{1}{|c|}{$n_s=1$} & $n_s=2$ & $n_s=4$ \\
    \hline
    \multirow{3}{*}{Analog prediction} & $n_s=1$ &\cellcolor[HTML]{202020} & $64.6$ & $40.6$\\
    \cline{2-5}
    & $n_s=2$  & $64.6$ & \cellcolor[HTML]{202020} & $25.8$\\
    \cline{2-5}
    & $n_s=4$ & $40.6$ & $25.8$ & \cellcolor[HTML]{202020}\\
    \hline
  \end{tabular}
  \caption{\acp{RMSE} of each analog prediction to the others depending on the serial size ($n_s$)}
  \label{tab:airlineAnalogError}
\end{table}

Once again, the results are quite close to each others, but as expected the errors (\cref{tab:airlineAnalogError}) are higher then the ones found in \cref{tab:airlineAnalogNoCZoomed}. This is due to the fact that the errors depend on the scale of the results, the predcitions here having a much greater range (\cref{tab:airlineAnalog}). The farthest appart ($n_s=1$ to $n_s=2$ with an error of $64.6$ thousands passengers) have an error barely higher than the error of digital results to the target dataset (\cref{tab:airlineDigital})

The overall observation of those results compared to the previous ones (\cref{subsec:airlineAnalogNoC}) are how much closer to their target they are. It still is not quite perfect but simply changing the activation functions used for training improved the results by a lot. It can thus be theorized that the error will greatly decrease if the circuit is trained with the same parameters as the ones it is using for inference. In other words, the error will probably be very low when the training will happen inSitu (\cref{subsec:inSitu}).

\subsubsection{\ac{GRU} predictions}

Like explained in \cref{sec:gruCircuit}, the \ac{GRU} layer cannot be serialized in an analog circuit. The simulation were only ran once, for the only version available.

\begin{figure}[H]
  \centering
  \includesvg[width=\linewidth]{results/airline/analogGRU}
  \caption{Analog prediction of the \ac{GRU} analog circuit}
  \label{graph:airlineAnalogGRU}
\end{figure}

\Cref{graph:airlineAnalogGRU} contains the analog predcitions of the \ac{GRU} circuit. The results are even worse than the results of simulation using weights generated with the default activation functions. Indeed, the error rate found in \cref{graph:airlineAnalogGRU} is much greater than the ones in \cref{tab:airlineAnalogNoC}. This means that while the circuit produces the right output shape, there is something wrong with the current version of the circuit.
