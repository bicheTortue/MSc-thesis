\section{Future work}\label{sec:further}

\subsection{Changing the verilog-A files for real circuit implementation}

Making the system actually one hundred percent implemented for inference is the obvious next step in making a fully analog \ac{LSTM} computer.

This means designing both an \ac{opAmp} for the current flowing through the circuit and a voltage multiplier working in the right voltage range.

\subsection{Circuit controller}\label{subsec:control}

The final circuit will need an external controller to manage diffenrent things in the circuit. This wasn't created in this thesis but will have to be in order to have a functionning circuit once fabricated %TODO ref annex

The controller will increase the power consumption of the overall system. However, due to the slow (precision of about $1\mu s$) nature of the system requirements, the controller will be very low power, and won't affect the overall power consumption of the final system.

The controller will have several roles in the system :
\begin{itemize}
  \item The first role will be to manage the different flags. This means that the controller will have to set those to high or low in order to control the system. This includes flags like the enable flags of the voltage driven crossbar array (\cref{subsec:serpar}) and those for the memory cell control.
  \item Anoter use for this controller is to reset the memory cell to $V_{cm}$ ($0$ as real value equivalent, \cref{tab:valConv}). This will be done by enabling the input and then applying $V_{cm}$ to the input, this will set the capacitor to $V_{cm}$. This has to be done everytime the circuit is ran.
  \item The controller will also be used to control the memristors internal resistance and control the weights. Depending on the type of memristor, the method and time to change the internal resistance differs. That will only be use for inSitu trainning(\cref{subsec:inSitu}).
\end{itemize}

\subsection{inSitu trainning}\label{subsec:inSitu}

Of course the first thing that comes to mind to improve the system and making it actually usable for a real world application would be doing inSitu training, meaning that the weight training would also be happening on the chip. Training on the chip also means that the weights, hence the memristors in the circuit, need to be changed between each epoch. That requires to have a backpropagation algorithm. Here, two options are available :

\begin{itemize}
  \item The easiest option would be to use the external controller (\cref{subsec:control}) to make the backprogation computation needed. This is not the best option, as the backpropagation would be happening in digital. This digital computation would have to happen as fast as the analog computation and thus a much faster controller would be required, meaning a bigger power consumption.
  \item The ideal way of implementing the backpropagation would be using an analog system similar to the one that is used for the inference. This would still mean having a controller to change the internal resistance of the memristors, this version would require a slower clock and making the controller lower powered. Using an analog implementation might also harm the system, as the backpropagation algorithm will probably be simpler and reduce the efficiency of the system.
\end{itemize}

In order for the inSitu training to work, the weights also need to change. Future work also also include looking for ways to change the weights in the fastest possible way. Idealy the algorithm won't need to compute the weight to resistance conversion (\cref{sec:wei2res}) and directly know how the change the resistances' values.

Training directly on the chip has a great advange over external training. Indeed training within the chip will allow to completely ignore the wires own resistance in the crossbar array. This is because they will an integral part of the weights, if the weight needs to be lowered then the resistance will be raised (because the conductances need to be lowered). %TODO finish with better explainnation

\subsubsection{Only change the value of one of the two memristor}

This is a potential idea on how to improve the time it takes to compute a single epoch. Indeed changing the resistance value in a memristor, takes time depending on the type of memristor that is being used. If the weight to resistance is computed the same way as in the thesis (\cref{sec:wei2res}), so that each pair of resistance is centered around the middle point of the memristor's resistance range.

Instead of having to do that, the idea is to only change one of the memristor's internal resistance so that the final weight is still the same. \Cref{eq:wei2res0} needs to be respected in any situation. The memristor's resistances won't be centered (\cref{eq:wei2res1}) anymore.

\subsection{Permanent training}

Once the training is fully implemented in an analog computer, the next step would be to find a way to permanently training the system. Meaning that any new information will be used to update the weights. This means that the system would need to know if the prediction it has made is correct or not.

A potential way to implement that, would be in a two phase plan. First the prediction, and so on until a verification on the predictions can be done. After that the weights are updated based on those previous reviewed results.

\subsection{Other \acp{LSTM} variants}

As discused before(\cref{sec:genwei}), they are only two of the \ac{LSTM} variants available in Keras. Nonetheless, it is possible to write custom layer function for Keras. Creating a layer from scratch is very time consuming and not so interresting for the scope the results needed. It can be done to extend the reach of the work.

\subsection{Avoiding doubling memory cells}\label{subsec:noDoubleMemcell}

Here there are two different problems that have two similar solutions, using a capacitor to keep the voltage in the node for a longer time :
\begin{itemize}
  \item For the hidden state feedback memory cells, a potential solution would be adding a single capacitor before the input pin of the memory cell and then enabling the input during the pause between time steps ($e_{next}$). This would allow the enable in and enable out flags to be high at different times. This solution only works in fully parallel mode.
  \item For the cell state memory cells, adding a capacitor and a buffer (\cref{ap:buffer}) after the output of the memory cell group. This capacitor is here to store the voltage after the output enable flag goes to low. The buffer is here to output the voltage with a high impedance.
\end{itemize}

Those solutions are purely theorical, they haven't been tested due to lack of time.

\subsection{Removing weight constraints}\label{subsec:noCons}

Removing the weights constraints is tricky since the the \ac{opAmp} is based on a verilog-A model (\cref{subsec:opamp}). Indeed, the model needs to be changed in order not to output voltages outside of the system's voltage range ($[0,v_{dd}]$). Once that is done, the \ac{opAmp} should behave more like a real one. The \acp{opAmp} should now be able to output a saturated signal.

However those saturated levels need to be added to the training of the weights. This can be done in Keras by creating a custom layer in the circuit that keeps the values within a the defined range ($[-9,9]$ for this thesis). The training will thus happen with the values being saturated as it would happen in the circuit.

This solution is not implemented in the circuit as the saturating signals might affect the real circuit. This is would allow the circuit to access a larger range of weights and make the training more performant.
